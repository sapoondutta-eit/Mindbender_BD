[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-2)))[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Processing"})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///home/marcus/Mindbender_BD/task16/build.sbt","languageId":"scala","version":1,"text":"name := \"StreamHandler\"\nversion := \"1.0\"\nscalaVersion := \"2.11.8\"\n// https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector\n\n\n\nlibraryDependencies ++= Seq(\n    \"org.apache.spark\" %% \"spark-core\" % \"2.3.2\" % \"provided\",\n     \"org.mongodb.spark\" % \"mongo-spark-connector_2.11\" % \"2.3.4\",\n    \"org.apache.spark\" %% \"spark-sql\" % \"2.3.2\" % \"provided\"\n)\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///home/marcus/Mindbender_BD/task16/src/main/scala/Steamhandler.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.types._\nimport com.mongodb.spark._\nimport com.mongodb.spark.sql._\nimport com.mongodb.spark.config._\n\n\n\n\n\ncase class stock_data(symbol:String,name:String,exchange:String,currency:String,datetime:String,open:String,\nhigh:String,low:String,close:String,volume:String,previous_close:String,change:String,percent_change:String,\naverage_volume:String,fifty_two_week:String)\n\n//case class StockData()\nobject StreamHandler {\n\tdef main(args: Array[String]) {\n \n        val spark = SparkSession\n\t\t\t.builder\n\t\t\t.appName(\"Stream Handler\")\n            .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/test.myCollection?readPreference=primaryPreferred\")\n            .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/twelve.stocks\")\n            .getOrCreate()\n\n\n        // val readConfig = ReadConfig(Map(\"uri\" -> \"mongodb://127.0.0.1/test.myCollection\"))\n        // val writeConfig = WriteConfig(Map(\"uri\" -> \"mongodb://127.0.0.1/test.myCollection\"))\n\n\n        val schema1 = StructType(List(\nStructField(\"symbol\", StringType, true),\nStructField(\"name\", StringType, true),\nStructField(\"exchange\", StringType, true),\nStructField(\"currency\", StringType, true),\nStructField(\"datetime\", StringType, true),\nStructField(\"open\",StringType, true),\nStructField(\"high\", StringType, true),\nStructField(\"low\", StringType, true),\nStructField(\"close\", StringType, true),\nStructField(\"volume\", StringType, true),\nStructField(\"previous_close\", StringType, true),\nStructField(\"change\", StringType, true),\nStructField(\"percent_change\",StringType, true),\nStructField(\"average_volume\", StringType, true),\nStructField(\"fifty_two_week\",StringType,true)\n)\n)\n\n\n        import spark.implicits._\n\n\n\n        val inputDF = spark\n\t\t\t.readStream\n\t\t\t.format(\"kafka\") \n\t\t\t.option(\"kafka.bootstrap.servers\", \"localhost:9092,localhost:9093,localhost:9094\")\n\t\t\t.option(\"subscribe\", \"stockmarket\")\n\t\t\t.load()\n            .select($\"value\" cast \"string\" as \"json\")\n            .select(from_json($\"json\", schema1) as \"data\")\n            .select(\"data.*\")\n\n        //val df = MongoSpark.load(spark)\n        //df.printSchema()\n\n\n\n        //val rawDF = inputDF.selectExpr(\"CAST(value AS STRING)\").as[String]\n\n        \n        // var stagedf = inputDF.toDF().coalesce(1)\n        // stagedf.printSchema()\n        //val newdf = inputDF.union(inputDF)\n\n        // MongoSpark.save(stagedf.writeStream.mode(SaveMode.Overwrite))\n//        MongoSpark.save(stagedf.write.option(\"collection\", \"stockdata\").mode(\"overwrite\"))\n\n//         val mongoDbFormat = \"com.stratio.datasource.mongodb\"\n//         val mongoDbDatabase = \"mongodatabase\"\n//         val mongoDbCollection = \"mongodf\"\n\n//         val mongoDbOptions = Map(\n//     MongodbConfig.Host -> \"localhost:27017\",\n//     MongodbConfig.Database -> mongoDbDatabase,\n//     MongodbConfig.Collection -> mongoDbCollection\n// )\n//     stagedf.write\n//     .format(mongoDbFormat)\n//     .mode(SaveMode.Append)\n//     .options(mongoDbOptions)\n//     .save()\n        \n\n        //inputDF.printSchema()                           \n        //inputDF.write.format(\"mongo\").mode(\"append\").save()\n        var query = inputDF\n        .writeStream\n        .outputMode(\"update\")\n        .option(\"truncate\", \"false\")\n        .format(\"console\")\n        .start()\n\n        val df = MongoSpark.load(sparkSession = spark).toDF()\n        MongoSpark.save(df)\n        //MongoSpark.save(df.write.option(\"collection\", \"stockmarket\").mode(\"overwrite\").format(\"mongo\"))\n\n\n\n\n\n        //MongoSpark.save(inputDF.write.mode(SaveMode.Overwrite))\n        //MongoSpark.save(inputDF.write.mode(\"overwrite\"), writeConfig)\n        //.trigger(ProcessingTime.create(\"40 seconds\"))\n        // var query = inputDF\n        // .writeStream\n        // .format(\"com.mongodb.spark.sql.DefaultSource\")\n        // .outputMode(\"append\")\n        // .start()\n\n\n\n        query.awaitTermination()\n    }\n    \n}"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (/home/marcus/Mindbender_BD/task16/target/scala-2.11/zinc/inc_compile_2.11.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed Dec 3, 2020 8:40:58 PM[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":4,"message":"Done"})[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
